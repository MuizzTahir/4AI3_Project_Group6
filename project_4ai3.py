# -*- coding: utf-8 -*-
"""Project_4AI3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ypmqWyL7ikRYDmPDapcZoUXNIEygNZcC

# SMRTTECH 4AI3 - Project (Group 6)

### APS System Failure

### Team:
- Muizz Raja (400198557)
- Walid Alshareef (400208789)

#Import Libraries
"""

# Import required libraries
import seaborn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from imblearn.over_sampling import SMOTE
from typing import Counter
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

"""#Data Visualization"""

# Read the csv file into a pandas dataframe
df_train = pd.read_csv('aps_failure_training_set.csv', na_values="na")

# Visualize neg vs pos data
seaborn.histplot(data = df_train, x = 'class')
plt.title("Negative vs Positive Data Samples")
plt.show()

# Visualize percentage of missing attributes by column
fig, ax = plt.subplots(figsize=(25,10))
null_data = df_train.isna().sum().div(df_train.shape[0]).mul(100).to_frame().sort_values(by=0, ascending = False)
ax.bar(null_data.index, null_data.values.T[0])
plt.xticks([])
plt.ylabel("Missing Values (%)")
plt.title("Percentage of Missing Values Across Attributes")
plt.show()

"""# Data Pre-Processing"""

# Assign neg and pos to 0 and 1 for easier classification
df_train['class'] = (df_train['class'] == 'pos').astype(int)

# Identify columns with over 50% data missing and drop them
missing_values = ((df_train.isna().sum())/df_train.shape[0]).sort_values(ascending=False)
above_50 = list(missing_values[missing_values > 0.5].index)
new_train = df_train.drop(columns=above_50)
new_train.shape

# Medain impute columns with below 50% data missing 
impute = lambda series: series.fillna(series.median())
new_train = new_train.transform(impute)

# Seperate features and class labels
x_train = new_train.drop('class',axis=1)
y_train = new_train['class']

# Normalize the data to bring into a similar range
scaler = MinMaxScaler()
x_scaled = pd.DataFrame(scaler.fit_transform(x_train), columns=x_train.columns)

# Feature selection using lasso logisitic regression
model = LogisticRegression(penalty='l1', solver='saga', max_iter=10000)
model.fit(x_scaled, y_train)
model.coef_

# Measure the score of the feature selection model
score = model.score(x_scaled, y_train)
print(f"The model has a performance of {score} on the training set.")

# Select the useful features
selected_features = x_scaled.columns[model.coef_[0] != 0]
x_selected = x_scaled[selected_features]
x_selected.shape

# Deal with imbalanced dataset
oversample = SMOTE(sampling_strategy = 0.25)
X, Y = oversample.fit_resample(x_selected, y_train)
counter = Counter(Y)
print(counter)

"""# Test Data Handling


"""

# Read the csv file into a pandas dataframe
df_test = pd.read_csv('aps_failure_test_set.csv', na_values="na")

# Visualize neg vs pos data
seaborn.histplot(data = df_test, x = 'class')
plt.title("Negative vs Positive Data Samples")
plt.show()

# Visualize percentage of missing attributes by column
fig, ax = plt.subplots(figsize=(25,10))
null_data_test = df_test.isna().sum().div(df_test.shape[0]).mul(100).to_frame().sort_values(by=0, ascending = False)
ax.bar(null_data_test.index, null_data_test.values.T[0])
plt.xticks([])
plt.ylabel("Missing Values (%)")
plt.title("Percentage of Missing Values Across Attributes")
plt.show()

# Assign neg and pos to 0 and 1 for easier classification
df_test['class'] = (df_test['class'] == 'pos').astype(int)

# Identify columns with over 50% data missing and drop them
missing_values_test = ((df_test.isna().sum())/df_test.shape[0]).sort_values(ascending=False)
above_50_test = list(missing_values_test[missing_values_test > 0.5].index)
new_test = df_test.drop(columns=above_50_test)
new_test.shape

# Medain impute columns with below 50% data missing using the train set median
new_test = new_test.transform(impute)

# Seperate features and class labels
x_test = new_test.drop('class',axis=1)
y_test = new_test['class']

# Normalize the data to bring into a similar range using normal transform
x_test_scaled = pd.DataFrame(scaler.transform(x_test), columns=x_test.columns)

# Testing the strength of features selected and resampling of underrepresented data on test data
x_test_selected = x_test_scaled[selected_features]
test_model = LogisticRegression(max_iter=10000)
test_model.fit(X, Y)
score = test_model.score(x_test_selected, y_test)
print(f"Our model had a performance of {score} on the test set.")

# Deal with imbalanced dataset
oversample = SMOTE(sampling_strategy = 0.25)
Xt, Yt = oversample.fit_resample(x_test_selected, y_test)
countert = Counter(Yt)
print(countert)

"""#Model

Model 1: Logistic Regression
"""

model1 = LogisticRegression(solver='liblinear')
model1.fit(X, Y)

predictions1 = model1.predict(Xt)
predictions1

percentage = model1.score(Xt, Yt)
percentage

"""Model 2: Random Forest Classifier"""

model2 = RandomForestClassifier(max_depth=5, random_state=10)
model2.fit(X, Y)

predictions2 = model2.predict(Xt)
predictions2

percentage = model2.score(Xt, Yt)
percentage

"""Model 3: KNeighbours"""

model3 = KNeighborsClassifier(n_neighbors=8)
model3.fit(X, Y)

predictions3 = model3.predict(Xt)
predictions3

percentage = model3.score(Xt, Yt)
percentage

"""#Hyper-Parameter Tuning

Model 1 parameter tuning and results
"""

params = {'C': 10**np.linspace(-5, 5, 11)}
clf = GridSearchCV(model1, params, n_jobs=-1, verbose=1)
clf.fit(X, Y)
bs = clf.best_score_
bp = clf.best_params_
be = clf.best_estimator_
predictions_hyper = be.predict(Xt)
print(f"The best model had the following parameters {bp} with this corresponding score {bs}")
tn, fp, fn, tp = confusion_matrix(Yt, predictions_hyper).ravel()
tpr = tp/(tp + fn)
tnr = tn/(tn + fp)
print(f"Our model performs with an accuracy of {tpr} on the positive class and {tnr} on the negative class.")

"""Model 2 parameter tuning and results"""

params = {'min_samples_split': [2, 4, 6, 8, 10], 
          'min_samples_leaf': [1, 2, 3, 4, 5]}
clf = GridSearchCV(model2, params, n_jobs=-1, verbose=1)
clf.fit(X, Y)
bs = clf.best_score_
bp = clf.best_params_
be = clf.best_estimator_
predictions_hyper = be.predict(Xt)
print(f"The best model had the following parameters {bp} with this corresponding score {bs}")
tn, fp, fn, tp = confusion_matrix(Yt, predictions_hyper).ravel()
tpr = tp/(tp + fn)
tnr = tn/(tn + fp)
print(f"Our model performs with an accuracy of {tpr} on the positive class and {tnr} on the negative class.")

"""Model 3 parameter tuning and results"""

params = {'n_neighbors': [5, 10, 15, 20, 25]}
clf = GridSearchCV(model3, params, n_jobs=-1, verbose=1)
clf.fit(X, Y)
bs = clf.best_score_
bp = clf.best_params_
be = clf.best_estimator_
predictions_hyper = be.predict(Xt)
print(f"The best model had the following parameters {bp} with this corresponding score {bs}")
tn, fp, fn, tp = confusion_matrix(Yt, predictions_hyper).ravel()
tpr = tp/(tp + fn)
tnr = tn/(tn + fp)
print(f"Our model performs with an accuracy of {tpr} on the positive class and {tnr} on the negative class.")

"""# References

datark1. (2021, June 6). Scania APS failures - A pipeline with PCA and ML. Kaggle. Retrieved
November 10, 2022, from
https://www.kaggle.com/code/datark1/scania-aps-failures-a-pipeline-with-pca-and-ml

Kavish111. (2022, May 30). Handling imbalanced data with imbalance-learn in Python. Analytics Vidhya. Retrieved November 29, 2022, from https://www.analyticsvidhya.com/blog/2022/05/handling-imbalanced-data-with-imbalance-learn-in-python/ 

Shetty, R. (2021, January 2). Predicting a failure in Scania’s air pressure system (APS).
Predicting a Failure in Scania’s Air Pressure System. Retrieved November 10, 2022, from
https://towardsdatascience.com/predicting-a-failure-in-scanias-air-pressure-system-aps-c260bcc4
d038

Sklearn.linear_model.logisticregression. scikit. (n.d.). Retrieved November 29, 2022, from https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html 

Sklearn.ensemble.randomforestclassifier. scikit. (n.d.). Retrieved November 29, 2022, from https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html 

Sklearn.neighbors.kneighborsclassifier. scikit. (n.d.). Retrieved November 29, 2022, from https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html 

Sklearn.model_selection.GRIDSEARCHCV. scikit. (n.d.). Retrieved December 6, 2022, from https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html# 

Smote#. SMOTE - Version 0.9.1. (n.d.). Retrieved December 4, 2022, from https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html
"""